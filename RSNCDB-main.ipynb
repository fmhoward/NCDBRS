{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84338bdb",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "First we have to import some packages to use down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "import sys\n",
    "from tableone import TableOne\n",
    "import sklearn\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, LassoLarsIC, LassoCV, LassoLarsCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.calibration import calibration_curve\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import imblearn\n",
    "import scipy\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import datetime\n",
    "import openpyxl\n",
    "import numpy.random as rng\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "from ncdb_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a66944",
   "metadata": {},
   "source": [
    "# Loading the NCDB File\n",
    "\n",
    "First step is to properly load the NCDB file.\n",
    "\n",
    "Since loading the full NCDB csv takes quite some time, you can utilize the savefile and loadfile specifications to save a subset of the database once you've narrowed down to the patients / variables you are interested in.\n",
    "\n",
    "The import function's name is load_data_her2. After the function, we specify parameters (filename, savefile, loadfile, and lower). You can provide default values for these parameters if you want using the equals sign - this means that the user doesn't need to provide these parameters every time the function is called.\n",
    "\n",
    "Detailed descriptions of all NCDB elements are found here:\n",
    "<a href=\"https://www.facs.org/-/media/files/quality-programs/cancer/ncdb/puf_data_dictionary_2017.ashx\">https://www.facs.org/-/media/files/quality-programs/cancer/ncdb/puf_data_dictionary_2017.ashx</a>\n",
    "\n",
    "https://www.facs.org/media/4tzpgfsb/ncdb-puf-quickstart-data-structure-2020.pdf\n",
    "\n",
    "'Site Specific Factors' are found here:\n",
    "<a href=\"https://web2.facs.org/cstage0205/breast/Breastschema.html\">https://web2.facs.org/cstage0205/breast/Breastschema.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9277535",
   "metadata": {},
   "source": [
    "Let's demonstrate the use of our above NCDB accessing function. Replace the filename with your copy of NCDB.\n",
    "\n",
    "This code will take some time to execute! Try not to rerun it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data_her2(\n",
    "    filename = r\"/mnt/data/NCDB/NCDBPUF_Breast.0.2020.csv\",\n",
    "    lower = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389498a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds the relevant important columns to the dataframe\n",
    "df_new = getNCDBClassifications(df, use_imputation = False)\n",
    "df_new = addTennNomogram(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27242f39",
   "metadata": {},
   "source": [
    "# Minimal Feature Set\n",
    "How can we logically exclude missing variables? Let's choose variables from the overall dataset that are 1) most predictive and 2) least missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = transformVar(df_new, 'age')\n",
    "df_new = transformVar(df_new, 'er_num')\n",
    "df_new = transformVar(df_new, 'pr_num')\n",
    "df_new = transformVar(df_new, 'ki67_num')\n",
    "df_new = transformVar(df_new, 'tumor_size')\n",
    "print(df_new.columns)\n",
    "\n",
    "maxList = ['age','sex', 'grade_med', 'grade_high', 'tumor_size', 'er', 'pr', 'regional_nodes_positive', 'chest_wall', 'skin_changes', 'inflammatory', 'lvi', \n",
    "                     'ductal', 'lobular', 'ductlob', 'mucinous', 'papillary', 'tubular', 'medullary', 'metaplastic', 'paget', 'sarcoma',\n",
    "                     'asian', 'black', 'hispanic', 'native', 'her2', 'er_num', 'pr_num', 'ki67_num', 'her2copies', 'her2ratio']\n",
    "#maxList += ['age_' + x for x in ['log', 'sq', 'sqrt', 'cbrt']]\n",
    "#maxList += ['er_num_' + x for x in ['log', 'sq', 'sqrt', 'cbrt']]\n",
    "#maxList += ['pr_num_' + x for x in ['log', 'sq', 'sqrt', 'cbrt']]\n",
    "#maxList += ['ki67_num_' + x for x in ['log', 'sq', 'sqrt', 'cbrt']]\n",
    "#maxList += ['tumor_size_' + x for x in ['log', 'sq', 'sqrt', 'cbrt']]\n",
    "\n",
    "mlf_trans = ['age_log','sex', 'grade_med', 'grade_high', 'tumor_size_log', 'er', 'pr', 'regional_nodes_positive', 'chest_wall', 'skin_changes', 'inflammatory', 'lvi', \n",
    "                     'ductal', 'lobular', 'ductlob', 'mucinous', 'papillary', 'tubular', 'medullary', 'metaplastic', 'paget', 'sarcoma',\n",
    "                     'asian', 'black', 'hispanic', 'native', 'her2', 'er_num_sq', 'pr_num', 'ki67_num_sqrt', 'her2copies', 'her2ratio']\n",
    "\n",
    "res = selectBaseInterestingFeatures(df_new, maxList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b23682",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sorted = dict(sorted(res.items(), key=lambda x:x[1]['llr'], reverse = True))\n",
    "n = len(res)\n",
    "width = 0.25\n",
    "  \n",
    "fig, ax1 = plt.subplots(dpi = 300)\n",
    "l1 = ax1.bar(np.arange(n), [x[1]['llr'] for x in res_sorted.items()], color = 'b',\n",
    "        width = width, edgecolor = 'black',\n",
    "        )\n",
    "ax1.set_xticks(np.arange(n) + width/2)\n",
    "ax1.set_xticklabels([x[0] for x in res_sorted.items()], rotation = 90, ha='center', size = 6)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "l2 = ax2.bar(np.arange(n) + width, [x[1]['sample'] for x in res_sorted.items()], color = 'g',\n",
    "        width = width, edgecolor = 'black',\n",
    "        )\n",
    "\n",
    "plt.legend([l1, l2], [\"LLR\", \"N\"])\n",
    "plt.xlabel(\"Feature\")\n",
    "ax1.set_ylabel(\"LLR\")\n",
    "ax2.set_ylabel(\"Sample Size\")\n",
    "\n",
    "plt.tight_layout()\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimal Feature Set:\n",
    "mfs = ['age','sex', 'grade_med', 'grade_high', 'tumor_size', 'er', 'pr', 'regional_nodes_positive', 'skin_changes', 'lvi', 'chest_wall',\n",
    "                     'ductal', 'lobular', 'ductlob', 'mucinous', 'tubular', 'medullary', 'metaplastic', 'paget', 'sarcoma', 'papillary', 'inflammatory',\n",
    "                     'hispanic', 'native', 'asian', 'black', 'her2', 'er_num', 'pr_num', 'ki67_num']\n",
    "\n",
    "#MFS with transforms\n",
    "mfs = ['age', 'age_log', 'age_sq', 'age_sqrt', 'age_cbrt', 'sex', 'grade_med', 'grade_high', \n",
    "                             'tumor_size', 'tumor_size_log', 'tumor_size_sq', 'tumor_size_sqrt', 'tumor_size_cbrt', 'er', 'pr', \n",
    "                             'regional_nodes_positive', 'skin_changes', 'lvi', 'chest_wall', 'ductal', 'lobular', 'ductlob', 'mucinous', 'tubular', \n",
    "                             'medullary', 'metaplastic', 'paget', 'sarcoma', 'papillary', 'inflammatory', 'hispanic', 'native', 'asian', 'black', 'her2', 'er_num_sq', 'pr_num', 'ki67_num_sqrt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6201a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d79268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_test_ncdb.alive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfe6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Excluding stage 0: \" + str(len(df_new.index)))\n",
    "\n",
    "df_train = df_new[(df_new.regional_nodes_positive < 4)]\n",
    "print(\"Excluding 4 or more nodes positive: \" + str(len(df_train.index)))\n",
    "\n",
    "df_train = df_train.dropna(subset = ['odx', 'high_odx'])\n",
    "print(\"Excluding patients missing ODX: \" + str(len(df_train.index)))\n",
    "\n",
    "df_train = df_train.dropna(subset = mfs)\n",
    "print(\"Excluding patients missing minimal feature set: \" + str(len(df_train.index)))\n",
    "\n",
    "#Add life expectancies\n",
    "df_train['life_expectancy'] = df_train.apply(lambda row: femaleLife[int(row['age'])] if row['sex'] == 0 else maleLife[int(row['age'])] if row['sex'] == 1 else None, axis=1)\n",
    "\n",
    "#Split into training and test sets\n",
    "df_train, df_test_ncdb = sklearn.model_selection.train_test_split(\n",
    "    df_train,\n",
    "    test_size = 0.2,\n",
    "    random_state = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 1000, n_jobs = -1)\n",
    "params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68280689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ncdb_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_features = make_best_model_features()\n",
    "\n",
    "for model_name in model_names:\n",
    "    best_model_features[model_name]['results'] = compareClassifiersNCDB(\n",
    "        df_train,\n",
    "        model = model,\n",
    "        params = params,\n",
    "        maximizeAIC = \"train\",\n",
    "        includeTransforms = \"all\",\n",
    "        doWarmStart = False,\n",
    "        **best_model_features[model_name]['arguments']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791c496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model_features = rename_models(best_model_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c453a13",
   "metadata": {},
   "source": [
    "# ROC Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names + ['Tennessee Nomogram']:\n",
    "    best_model_features[model_name]['subgroup_analyses'] = {\n",
    "        'histologic': None,\n",
    "        'race': None,\n",
    "        'nodes': {\n",
    "            'Negative': {},\n",
    "            'Positive': {}\n",
    "        },\n",
    "        'training': {\n",
    "            'all': {},\n",
    "            'test': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    best_model_features[model_name]['subgroup_analyses']['histologic'] = {k:{} for k in histologic_dict.keys()}\n",
    "    best_model_features[model_name]['subgroup_analyses']['race'] = {k:{} for k in race_dict.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05252340",
   "metadata": {},
   "source": [
    "## NCDB Model Assessment\n",
    "Using a restricted dataset that contains all the features of interest across all compared models, compare the AUC in the NCDB cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f97a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all of training data to determine sens/spec thresholds\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results'] = plotAUROCFeat(\n",
    "        ax,\n",
    "        df_train,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        average = True\n",
    "    )\n",
    "    \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only test subset to determine AUROCs and plot\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "    \n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['training']['test']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train, df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d0069",
   "metadata": {},
   "source": [
    "## UCMC Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501924fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc = pd.read_csv(r\"/mnt/data/UCMC/RS_dataset.csv\", dtype = {'nodegrp': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcba0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc_parse = getUCMCClassifications(df_ucmc, use_imputation = False)\n",
    "#df_ucmc_parse = addTennNomogram_UCMC(df_ucmc_parse, imputeMissing = False)\n",
    "df_ucmc_parse = transformVar(df_ucmc_parse, 'age')\n",
    "df_ucmc_parse = transformVar(df_ucmc_parse, 'er_num')\n",
    "df_ucmc_parse = transformVar(df_ucmc_parse, 'pr_num')\n",
    "df_ucmc_parse = transformVar(df_ucmc_parse, 'ki67_num')\n",
    "df_ucmc_parse = transformVar(df_ucmc_parse, 'tumor_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only including patients with < 4 lymph nodes positive and at least ER or PR positive\n",
    "df_ucmc_parse = df_ucmc_parse.loc[\n",
    "    (df_ucmc_parse.regional_nodes_positive < 4) &\n",
    "    ((df_ucmc_parse.er == 1) | (df_ucmc_parse.pr == 1))\n",
    "]\n",
    "\n",
    "#drop patients with missing data\n",
    "for model_name in model_names:\n",
    "    df_ucmc_parse = df_ucmc_parse.dropna(subset = best_model_features[model_name]['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc_parse_odx_val = df_ucmc_parse.dropna(subset = ['high_odx'])\n",
    "for model_name in model_names:\n",
    "    df_ucmc_parse_odx_val = df_ucmc_parse_odx_val.dropna(subset = best_model_features[model_name]['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc_parse_survival_val = df_ucmc_parse.copy()\n",
    "for model_name in model_names:\n",
    "    df_ucmc_parse_survival_val = df_ucmc_parse_survival_val.dropna(subset = best_model_features[model_name]['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate models\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external']['senspec_results'] = plotAUROCFeatExternal(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_ucmc_parse_odx_val,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf3c45",
   "metadata": {},
   "source": [
    "Model assessment comparison, including DL Path predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc_parse_odx_val_dl = df_ucmc_parse.dropna(subset = ['high_odx', 'percent_tiles_positive0'])\n",
    "for model_name in model_names:\n",
    "    df_ucmc_parse_odx_val_dl = df_ucmc_parse_odx_val_dl.dropna(subset = best_model_features[model_name]['features'])\n",
    "\n",
    "#evaluate models\n",
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external']['senspec_results'] = plotAUROCFeatExternal(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_ucmc_parse_odx_val_dl,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "best_model_features['DL Path'] = {}\n",
    "best_model_features['DL Path']['subgroup_analyses'] = {}\n",
    "best_model_features['DL Path']['subgroup_analyses']['external'] = {}\n",
    "best_model_features['DL Path']['subgroup_analyses']['external']['external'] = {}\n",
    "best_model_features['DL Path']['subgroup_analyses']['external']['external']['senspec_results'] = plotAUROCFeatExternal(\n",
    "     ax,\n",
    "     df_train,\n",
    "     df_ucmc_parse_odx_val_dl,\n",
    "     ['comb'],\n",
    "     label = \"DL Path\",\n",
    "     useColumn = 'comb'\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af166c",
   "metadata": {},
   "source": [
    "Using DeLong's method, compute z-scores and p-values for difference in AUC of models in the validation cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a6095",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_compare_z = pd.DataFrame(columns = model_names, index = model_names)\n",
    "auc_compare_pval = pd.DataFrame(columns = model_names, index = model_names)\n",
    "\n",
    "for m1 in model_names:\n",
    "    for m2 in model_names:\n",
    "        auc_compare_z.loc[m1, m2], auc_compare_pval.loc[m1, m2] = AUROCFeatExternal_zp(\n",
    "            df_train,\n",
    "            df_ucmc_parse_odx_val,\n",
    "            feat = best_model_features[m1]['features'],\n",
    "            feat_comparison = best_model_features[m2]['features'],\n",
    "            model = model\n",
    "        )\n",
    "        \n",
    "auc_compare_pval = 10**auc_compare_pval #initially returned as log10(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auc_compare_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76374f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_compare_z = pd.DataFrame(columns = model_names, index = model_names)\n",
    "auc_compare_pval = pd.DataFrame(columns = model_names, index = model_names)\n",
    "\n",
    "for m1 in model_names:\n",
    "    for m2 in model_names:\n",
    "        auc_compare_z.loc[m1, m2], auc_compare_pval.loc[m1, m2] = AUROCFeatExternal_zp(\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            feat = best_model_features[m1]['features'],\n",
    "            feat_comparison = best_model_features[m2]['features'],\n",
    "            model = model\n",
    "        )\n",
    "        \n",
    "auc_compare_pval_test = 10**auc_compare_pval #initially returned as log10(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auc_compare_pval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719514a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34e33147",
   "metadata": {},
   "source": [
    "## Race Subgroup Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e58371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Figures\n",
    "for model_name in model_name_legend_label_dict.keys():\n",
    "    fig, ax = plt.subplots(dpi = 300)\n",
    "    \n",
    "    for race_name, col_id in race_dict.items():    \n",
    "        print(race_name, model_name)\n",
    "        best_model_features[model_name]['subgroup_analyses']['race'][race_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            best_model_features[model_name]['features'],\n",
    "            label = race_name,\n",
    "            plot_color = race_colors_dict[race_name],\n",
    "            subgroup_type = 'race',\n",
    "            subgroup_id = col_id,\n",
    "            thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "        )\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Panel Figure\n",
    "fig, axs = plt.subplots(2, 2, dpi = 300)\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel()):\n",
    "    for race_name, col_id in race_dict.items():\n",
    "        print(race_name, model_name)\n",
    "        best_model_features[model_name]['subgroup_analyses']['race'][race_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            best_model_features[model_name]['features'],\n",
    "            label = race_name,\n",
    "            auc_in_legend = False,\n",
    "            plot_color = race_colors_dict[race_name],\n",
    "            subgroup_type = 'race',\n",
    "            subgroup_id = col_id,\n",
    "            thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "        )\n",
    "        \n",
    "    ax.title.set_text(model_name)\n",
    "        \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc='right', bbox_to_anchor=(1.35,0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f51017",
   "metadata": {},
   "source": [
    "## Histologic Subgroup Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d21d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sample_size = 50 # enforcing minimum sample size for testing subgroup performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "for histo_name, col_id in histologic_dict.items():\n",
    "    df_subgroup = df_train[\n",
    "        df_train.histology == col_id\n",
    "    ]\n",
    "    \n",
    "    print(histo_name, \"Tennessee Nomogram\")\n",
    "    \n",
    "    if (df_subgroup.shape[0] > min_sample_size) & (df_subgroup['high_odx'].sum() > 1):\n",
    "        best_model_features['Tennessee Nomogram']['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            ['ten_score'],\n",
    "            label = histo_name,\n",
    "            useColumn = 'ten_score',\n",
    "            plot_color = histologic_colors_dict[histo_name],\n",
    "            subgroup_type = 'histologic',\n",
    "            subgroup_id = col_id,\n",
    "            thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "        )\n",
    "    else:\n",
    "        best_model_features['Tennessee Nomogram']['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = None\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_name_legend_label_dict.keys():\n",
    "    fig, ax = plt.subplots(dpi = 300)\n",
    "    \n",
    "    for histo_name, col_id in histologic_dict.items():\n",
    "        df_subgroup = df_train[\n",
    "            df_train.histology == col_id\n",
    "        ]\n",
    "    \n",
    "        print(histo_name, model_name)\n",
    "        \n",
    "        if (df_subgroup.shape[0] > min_sample_size) & (df_subgroup['high_odx'].sum() > 1):\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "                ax,\n",
    "                df_train,\n",
    "                df_test_ncdb,\n",
    "                best_model_features[model_name]['features'],\n",
    "                label = histo_name,\n",
    "                plot_color = histologic_colors_dict[histo_name],\n",
    "                subgroup_type = 'histologic',\n",
    "                subgroup_id = col_id,\n",
    "                thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "            )\n",
    "        else:\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = None\n",
    "            \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c698a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Panel Figure\n",
    "fig, axs = plt.subplots(2, 2, dpi = 300)\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel()):\n",
    "    for histo_name, col_id in histologic_dict.items():  \n",
    "        df_subgroup = df_train[\n",
    "            df_train.histology == col_id\n",
    "        ]\n",
    "        \n",
    "        print(histo_name, model_name)\n",
    "        \n",
    "        if (df_subgroup.shape[0] > min_sample_size) & (df_subgroup['high_odx'].sum() > 1):\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "                ax,\n",
    "                df_train,\n",
    "                df_test_ncdb,\n",
    "                best_model_features[model_name]['features'],\n",
    "                label = histo_name,\n",
    "                auc_in_legend = False,\n",
    "                plot_color = histologic_colors_dict[histo_name],\n",
    "                subgroup_type = 'histologic',\n",
    "                subgroup_id = col_id,\n",
    "                thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "            )\n",
    "        else:\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = None\n",
    "            \n",
    "    ax.title.set_text(model_name)\n",
    "    \n",
    "handles, labels = axs.ravel()[0].get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc='right', bbox_to_anchor=(1.35,0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f0dd9",
   "metadata": {},
   "source": [
    "## Node Subgroup Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b58c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "print(\"Negative\", \"Tennesse Nomogram\")\n",
    "\n",
    "best_model_features['Tennessee Nomogram']['subgroup_analyses']['nodes']['Negative']['senspec_results'] = plotAUROCFeat_Test(\n",
    "    ax,\n",
    "    df_train,\n",
    "    df_test_ncdb,\n",
    "    ['ten_score'],\n",
    "    label = \"Negative\",\n",
    "    useColumn = 'ten_score',\n",
    "    subgroup_type = 'nodes',\n",
    "    subgroup_id = 0,\n",
    "    thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    ")\n",
    "\n",
    "print(\"Positive\", \"Tennessee Nomogram\")\n",
    "\n",
    "best_model_features['Tennessee Nomogram']['subgroup_analyses']['nodes']['Positive']['senspec_results'] = plotAUROCFeat_Test(\n",
    "    ax,\n",
    "    df_train,\n",
    "    df_test_ncdb,\n",
    "    ['ten_score'],\n",
    "    label = \"Positive\",\n",
    "    useColumn = 'ten_score',\n",
    "    subgroup_type = 'nodes',\n",
    "    subgroup_id = 1,\n",
    "    thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667911fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_name_legend_label_dict.keys():\n",
    "    fig, ax = plt.subplots(dpi = 300)\n",
    "\n",
    "    print(\"Negative\", model_name)\n",
    "\n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Negative']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = \"Negative\",\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 0,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "    \n",
    "    print(\"Positive\", model_name)\n",
    "\n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Positive']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = \"Positive\",\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 1,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b063ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Panel Figure\n",
    "fig, axs = plt.subplots(2, 2, dpi = 300)\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel()):\n",
    "    # Negative\n",
    "    print(\"Negative\", model_name)\n",
    "\n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Negative']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        auc_in_legend = False,\n",
    "        label = \"Negative\",\n",
    "        plot_color = nodes_colors_dict['Negative'],\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 0,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "    \n",
    "    # Postive Post-Menopausal\n",
    "    print(\"Positive\", model_name)\n",
    "    \n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Positive']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        auc_in_legend = False,\n",
    "        label = \"Positive\",\n",
    "        plot_color = nodes_colors_dict['Positive'],\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 1,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "    ax.title.set_text(model_name)\n",
    "        \n",
    "handles, labels = axs.ravel()[0].get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc='right', bbox_to_anchor=(1.35,0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 Panel Figure\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, dpi = 300, figsize = (10,8), sharex = True, sharey = True)\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel(order = 'F')[:3]):\n",
    "    for race_name, col_id in race_dict.items():\n",
    "        print(race_name, model_name)\n",
    "        best_model_features[model_name]['subgroup_analyses']['race'][race_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            best_model_features[model_name]['features'],\n",
    "            label = race_name,\n",
    "            auc_in_legend = False,\n",
    "            plot_color = race_colors_dict[race_name],\n",
    "            subgroup_type = 'race',\n",
    "            subgroup_id = col_id,\n",
    "            thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "        )\n",
    "        \n",
    "    ax.title.set_text(model_name_legend_label_dict[model_name])\n",
    "        \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc = 'upper left', bbox_to_anchor=(0.035,0))\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel(order = 'F')[3:6]):\n",
    "    for histo_name, col_id in histologic_dict.items():  \n",
    "        df_subgroup = df_train[\n",
    "            df_train.histology == col_id\n",
    "        ]\n",
    "        \n",
    "        print(histo_name, model_name)\n",
    "        \n",
    "        if (df_subgroup.shape[0] > min_sample_size) & (df_subgroup['high_odx'].sum() > 1):\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = plotAUROCFeat_Test(\n",
    "                ax,\n",
    "                df_train,\n",
    "                df_test_ncdb,\n",
    "                best_model_features[model_name]['features'],\n",
    "                label = histo_name,\n",
    "                auc_in_legend = False,\n",
    "                plot_color = histologic_colors_dict[histo_name],\n",
    "                subgroup_type = 'histologic',\n",
    "                subgroup_id = col_id,\n",
    "                thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "            )\n",
    "        else:\n",
    "            best_model_features[model_name]['subgroup_analyses']['histologic'][histo_name]['senspec_results'] = None\n",
    "            \n",
    "    ax.title.set_text(model_name_legend_label_dict[model_name])\n",
    "    \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc = 'upper left', bbox_to_anchor=(0.36,0))\n",
    "\n",
    "for model_name, ax in zip(model_name_legend_label_dict.keys(), axs.ravel(order = 'F')[6:]):\n",
    "    # Negative\n",
    "    print(\"Negative\", model_name)\n",
    "\n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Negative']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        auc_in_legend = False,\n",
    "        label = \"Negative\",\n",
    "        plot_color = nodes_colors_dict['Negative'],\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 0,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "    # Postive Post-Menopausal\n",
    "    print(\"Positive\", model_name)\n",
    "\n",
    "    best_model_features[model_name]['subgroup_analyses']['nodes']['Positive']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        auc_in_legend = False,\n",
    "        label = \"Positive\",\n",
    "        plot_color = nodes_colors_dict['Positive'],\n",
    "        subgroup_type = 'nodes',\n",
    "        subgroup_id = 1,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "    ax.title.set_text(model_name_legend_label_dict[model_name])\n",
    "        \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc = 'upper left', bbox_to_anchor=(0.68,0))\n",
    "\n",
    "plt.figtext(0.188, 1.0005, 'Race', fontweight = \"bold\", size = 12, ha = 'center')\n",
    "plt.figtext(0.516, 1.0005, 'Histology', fontweight = \"bold\", size = 12, ha = 'center')\n",
    "plt.figtext(0.825, 1.0005, 'Nodes', fontweight = \"bold\", size = 12, ha = 'center')\n",
    "\n",
    "for ax, l in zip(axs.ravel(order = 'F'), ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I']):\n",
    "    ax.set_title(l, loc='left', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f02195",
   "metadata": {},
   "source": [
    "## Compiling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df = pd.DataFrame(\n",
    "    columns = [\n",
    "        'model',\n",
    "        'subgroup_type',\n",
    "        'subgroup_group',\n",
    "        'n_patients',\n",
    "        'features',\n",
    "        'n_features',\n",
    "        '95pSens_thres',\n",
    "        '95pSens_sens',\n",
    "        '95pSens_spec',\n",
    "        '95pSpec_thres',\n",
    "        '95pSpec_sens',\n",
    "        '95pSpec_spec',\n",
    "        'AUC',\n",
    "        'AUC_lowerbound',\n",
    "        'AUC_upperbound',\n",
    "        'AUPRC',\n",
    "        'AUPRC_lowerbound',\n",
    "        'AUPRC_upperbound'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for model_name in model_names:\n",
    "    for subgroup_type in best_model_features[model_name]['subgroup_analyses'].keys():\n",
    "        for subgroup_group in best_model_features[model_name]['subgroup_analyses'][subgroup_type].keys():\n",
    "            print(model_name, subgroup_type, subgroup_group)\n",
    "            results = best_model_features[model_name]['subgroup_analyses'][subgroup_type][subgroup_group]['senspec_results']\n",
    "            \n",
    "            if results != None:\n",
    "                subgroup_results_df = subgroup_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'subgroup_type': subgroup_type,\n",
    "                    'subgroup_group': subgroup_group,\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    '95pSens_thres': results['95% Sensitivity']['Threshold'],\n",
    "                    '95pSens_sens': results['95% Sensitivity']['Sensitivity'],\n",
    "                    '95pSens_spec': results['95% Sensitivity']['Specificity'],\n",
    "                    '95pSpec_thres': results['95% Specificity']['Threshold'],\n",
    "                    '95pSpec_sens': results['95% Specificity']['Sensitivity'],\n",
    "                    '95pSpec_spec': results['95% Specificity']['Specificity'],\n",
    "                    'AUC': results['AUC']['AUC'],\n",
    "                    'AUC_lowerbound': results['AUC']['Lower Bound'],\n",
    "                    'AUC_upperbound': results['AUC']['Upper Bound'],\n",
    "                    'AUPRC': results['AUPRC']['AUPRC'],\n",
    "                    'AUPRC_lowerbound': results['AUPRC']['Lower Bound'],\n",
    "                    'AUPRC_upperbound': results['AUPRC']['Upper Bound']\n",
    "                }, ignore_index = True)\n",
    "            else:\n",
    "                subgroup_results_df = subgroup_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'subgroup_type': subgroup_type,\n",
    "                    'subgroup_group': subgroup_group,\n",
    "                    'n_patients': None,\n",
    "                    'features': None,\n",
    "                    'n_features': None,\n",
    "                    '95pSens_thres': None,\n",
    "                    '95pSens_sens': None,\n",
    "                    '95pSens_spec': None,\n",
    "                    '95pSpec_thres': None,\n",
    "                    '95pSpec_sens': None,\n",
    "                    '95pSpec_spec': None,\n",
    "                    'AUC': None,\n",
    "                    'AUC_lowerbound': None,\n",
    "                    'AUC_upperbound': None,\n",
    "                    'AUPRC': None,\n",
    "                    'AUPRC_lowerbound': None,\n",
    "                    'AUPRC_upperbound': None\n",
    "                }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df['auc_lb'] = subgroup_results_df['AUC'] - subgroup_results_df['AUC_lowerbound']\n",
    "subgroup_results_df['auc_ub'] = subgroup_results_df['AUC_upperbound'] - subgroup_results_df['AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_errbar(\n",
    "    data = subgroup_results_df.dropna(subset = ['AUC']),\n",
    "    subgroup_type = 'race'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df[\n",
    "    subgroup_results_df.subgroup_type == 'race'\n",
    "].pivot(\n",
    "    values = ['n_patients', 'AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['subgroup_group'],\n",
    "    index = ['model']\n",
    ").swaplevel(0, 1, axis = 1).sort_index(axis = 1).loc[model_names].transpose().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974941c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_errbar(\n",
    "    data = subgroup_results_df.dropna(subset = ['AUC']),\n",
    "    subgroup_type = 'histologic'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df[\n",
    "    subgroup_results_df.subgroup_type == 'histologic'\n",
    "].pivot(\n",
    "    values = ['n_patients', 'AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['subgroup_group'],\n",
    "    index = ['model']\n",
    ").swaplevel(0, 1, axis = 1).sort_index(axis = 1).loc[model_names].transpose().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9627a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_errbar(\n",
    "    data = subgroup_results_df.dropna(subset = ['AUC']),\n",
    "    subgroup_type = 'nodes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8827f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df[\n",
    "    subgroup_results_df.subgroup_type == 'nodes'\n",
    "].pivot(\n",
    "    values = ['n_patients', 'AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['subgroup_group'],\n",
    "    index = ['model']\n",
    ").swaplevel(0, 1, axis = 1).sort_index(axis = 1).loc[model_names].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a0dbf",
   "metadata": {},
   "source": [
    "## Generating Baseline Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineCharacteristics(pd.concat([df_train, df_test_ncdb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1684430",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baselineCharacteristics_external(df_ucmc_parse_survival_val.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0e8d1",
   "metadata": {},
   "source": [
    "## Manuscript Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi = 300, figsize=(1.5, 1.5))\n",
    "    \n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['training']['test']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.xaxis.set_tick_params(labelbottom=False)\n",
    "ax.yaxis.set_tick_params(labelleft=False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7853756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate models\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 300, figsize=(1.5, 1.5))\n",
    "    \n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external']['senspec_results'] = plotAUROCFeatExternal(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_ucmc_parse_odx_val,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('1 - Specificity')\n",
    "ax.set_ylabel('Sensitivity')\n",
    "ax.xaxis.set_tick_params(labelbottom=False)\n",
    "ax.yaxis.set_tick_params(labelleft=False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc855e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, dpi = 300, figsize = (10.5, 4), sharey = True)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['training']['test']['senspec_results'] = plotAUROCFeat_Test(\n",
    "        ax[0],\n",
    "        df_train,\n",
    "        df_test_ncdb,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax[0].set_title('A', loc='left', fontweight='bold')\n",
    "ax[0].set_xlabel('1 - Specificity')\n",
    "ax[0].set_ylabel('Sensitivity')\n",
    "ax[0].legend()\n",
    "    \n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    best_model_features[model_name]['subgroup_analyses']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external'] = {}\n",
    "    best_model_features[model_name]['subgroup_analyses']['external']['external']['senspec_results'] = plotAUROCFeatExternal(\n",
    "        ax[1],\n",
    "        df_train,\n",
    "        df_ucmc_parse_odx_val,\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        model = model,\n",
    "        thresholds = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']\n",
    "    )\n",
    "\n",
    "ax[1].set_title('B', loc='left', fontweight='bold')\n",
    "ax[1].set_xlabel('1 - Specificity')\n",
    "ax[1].set_ylabel('Sensitivity')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tick_params('y', labelleft=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_results_data = {k:{c:{'95% Sensitivity': None, '90% Sensitivity': None} for c in model_names} for k in ['training', 'validation']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Validation Survival Curves in High/Low ODX at 90% Sensitivity Cutoff\n",
    "\n",
    "cutoff_level = '90% Sensitivity'\n",
    "cutoff = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results'][cutoff_level]['Threshold']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, dpi = 300, figsize = (8,12), sharey = True, sharex = True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for h_odx in [0, 1]:\n",
    "    for model_name in model_names:\n",
    "        print(cutoff_level, model_name)\n",
    "        ax = axs.ravel(order = 'F')[i]\n",
    "        try:\n",
    "            survival_results_data['validation'][model_name][cutoff_level] = plotSurvivalFeatExternal(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_ucmc_parse_survival_val[(df_ucmc_parse_survival_val.regional_nodes_positive < 4) & (df_ucmc_parse_survival_val.high_odx == h_odx)],\n",
    "            best_model_features[model_name]['features'],\n",
    "            label = model_name_legend_label_dict[model_name],\n",
    "            cutoff = cutoff,\n",
    "            model = model\n",
    "            )\n",
    "        \n",
    "            if survival_results_data['validation'][model_name][cutoff_level]['Univariate']['HR']['Upper Bound'] > 10000:\n",
    "                ax.annotate(\"HR: N/A\", (0, 0.61))\n",
    "            else:\n",
    "                ax.annotate(stringHR_from_data(survival_results_data['validation'][model_name][cutoff_level]['Univariate']), (0, 0.61))\n",
    "        except:\n",
    "            pass\n",
    "        i = i + 1\n",
    "        \n",
    "for ax, l in zip(axs.ravel(order = 'F'), ['A', 'B', 'C', 'D', 'E', 'F']):\n",
    "    try:\n",
    "        ax.get_legend().remove()\n",
    "    except:\n",
    "        pass\n",
    "    ax.set_title(l, loc='left', fontweight='bold')\n",
    "    ax.set_ylim([0.60, None])\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Recurrence Free Interval')\n",
    "\n",
    "plt.figtext(0.25, 1.0005, 'Low ODX', fontweight = \"bold\", size = 12)\n",
    "plt.figtext(0.75, 1.0005, 'High ODX', fontweight = \"bold\", size = 12)\n",
    "axs[0,1].legend(['Low Risk', 'High Risk'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444db08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Validation Survival Curves\n",
    "\n",
    "cutoff_levels = [\n",
    "    '95% Sensitivity',\n",
    "    '90% Sensitivity'\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, dpi = 300, figsize = (8,12), sharey = True, sharex = True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for cutoff_level in cutoff_levels:    \n",
    "    for model_name in model_names:\n",
    "        cutoff = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results'][cutoff_level]['Threshold']\n",
    "\n",
    "        print(cutoff_level, cutoff, model_name)\n",
    "        ax = axs.ravel(order = 'F')[i]\n",
    "        \n",
    "        survival_results_data['validation'][model_name][cutoff_level] = plotSurvivalFeatExternal(\n",
    "        ax,\n",
    "        df_train,\n",
    "        df_ucmc_parse_survival_val[(df_ucmc_parse_survival_val.regional_nodes_positive < 4)],\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        cutoff = cutoff,\n",
    "        model = model\n",
    "        )\n",
    "        i = i + 1\n",
    "        \n",
    "        if math.isinf(survival_results_data['validation'][model_name][cutoff_level]['Univariate']['HR']['Upper Bound']):\n",
    "            ax.annotate(\"HR: N/A\", (0, 0.905))\n",
    "        else:\n",
    "            ax.annotate(stringHR_from_data(survival_results_data['validation'][model_name][cutoff_level]['Univariate']), (0, 0.905))\n",
    "\n",
    "for ax, l in zip(axs.ravel(order = 'F'), ['A', 'B', 'C', 'D', 'E', 'F']):\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_title(l, loc='left', fontweight='bold')\n",
    "    ax.set_ylim([0.90, None])\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Recurrence Free Interval')\n",
    "\n",
    "plt.figtext(0.22, 1.0005, '95% Sensitivity', fontweight = \"bold\", size = 12)\n",
    "plt.figtext(0.71, 1.0005, '90% Sensitivity', fontweight = \"bold\", size = 12)\n",
    "axs[0,1].legend(['Low Risk', 'High Risk'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47270729",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Survival Curves\n",
    "\n",
    "cutoff_levels = [\n",
    "    '95% Sensitivity',\n",
    "    '90% Sensitivity'\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, dpi = 300, figsize = (8,12), sharey = True, sharex = True)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for cutoff_level in cutoff_levels:\n",
    "    for model_name in model_names:\n",
    "        cutoff = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results'][cutoff_level]['Threshold']\n",
    "        print(cutoff_level, cutoff, model_name)\n",
    "        \n",
    "        ax = axs.ravel(order = 'F')[i]\n",
    "        \n",
    "        survival_results_data['training'][model_name][cutoff_level] = plotSurvivalFeat_Test(\n",
    "            ax,\n",
    "            df_train,\n",
    "            df_test_ncdb,\n",
    "            best_model_features[model_name]['features'],\n",
    "            label = model_name_legend_label_dict[model_name],\n",
    "            cutoff = cutoff\n",
    "        )\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "        if math.isinf(survival_results_data['training'][model_name][cutoff_level]['OS']['HR']['Upper Bound']):\n",
    "            ax.annotate(\"HR: N/A\", (0, 0.938))\n",
    "        else:\n",
    "            ax.annotate(stringHR_from_data(survival_results_data['training'][model_name][cutoff_level]['OS']), (0, 0.938))\n",
    "\n",
    "for ax, l in zip(axs.ravel(order = 'F'), ['A', 'B', 'C', 'D', 'E', 'F']):\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_title(l, loc='left', fontweight='bold')\n",
    "    ax.set_ylim([0.935, None])\n",
    "    ax.yaxis.set_tick_params(labelleft = True)\n",
    "    ax.set_xlabel('Months')\n",
    "    ax.set_ylabel('Overall Survival')\n",
    "\n",
    "\n",
    "plt.figtext(0.22, 1.0005, '95% Sensitivity', fontweight = \"bold\", size = 12)\n",
    "plt.figtext(0.71, 1.0005, '90% Sensitivity', fontweight = \"bold\", size = 12)\n",
    "axs[0,1].legend(['Low Risk', 'High Risk'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, dpi = 300, figsize = (10, 5), sharey = True)\n",
    "i = 0\n",
    "j = 0\n",
    "for model_name in model_names[1:]:\n",
    "    plotSurvivalFeatExternal_Path(\n",
    "        ax[i],\n",
    "        df_train,\n",
    "        df_ucmc_parse_survival_val[(df_ucmc_parse_survival_val.regional_nodes_positive < 4)],\n",
    "        best_model_features[model_name]['features'],\n",
    "        label = model_name_legend_label_dict[model_name],\n",
    "        cutoff = best_model_features[model_name]['subgroup_analyses']['training']['all']['senspec_results']['95% Sensitivity']['Threshold'],\n",
    "        model = model\n",
    "    )\n",
    "    ax[i].set_ylim([0.86, 1.005])\n",
    "    ax[i].legend()\n",
    "    i = i + 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a573d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, dpi = 300, figsize = (8,10))\n",
    "\n",
    "i = 0\n",
    "for model_name in model_names:\n",
    "    linearPlot(\n",
    "        df_test_ncdb,\n",
    "        df_train,\n",
    "        LogisticRegression(max_iter = 1000, n_jobs = -1),\n",
    "        use_weights = False,\n",
    "        features = best_model_features[model_name]['features'],\n",
    "        ax = axs.ravel(order = 'F')[i],\n",
    "        title = model_name_legend_label_dict[model_name]\n",
    "    )\n",
    "    i = i + 1\n",
    "    \n",
    "for model_name in model_names:\n",
    "    calibrationPlot(\n",
    "        df_test_ncdb,\n",
    "        df_train,\n",
    "        LogisticRegression(max_iter = 1000, n_jobs = -1),\n",
    "        use_weights = False,\n",
    "        features = best_model_features[model_name]['features'],\n",
    "        ax = axs.ravel(order = 'F')[i],\n",
    "        title = model_name_legend_label_dict[model_name]\n",
    "    )\n",
    "    i = i + 1\n",
    "    \n",
    "for ax, l in zip(axs.ravel(order = 'F'), ['A', 'B', 'C', 'D', 'E', 'F']):\n",
    "    ax.set_title(l, loc='left', fontweight='bold')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac46b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sorted = dict(sorted(res.items(), key=lambda x:x[1]['llr'], reverse = True))\n",
    "\n",
    "n = len(res)\n",
    "width = 0.25\n",
    "  \n",
    "fig, ax1 = plt.subplots(dpi = 300, figsize = (10, 4.5))\n",
    "l1 = ax1.bar(np.arange(n), [x[1]['llr'] for x in res_sorted.items()], color = 'b',\n",
    "        width = width, edgecolor = 'black',\n",
    "        )\n",
    "ax1.set_xticks(np.arange(n) + width/2)\n",
    "ax1.set_xticklabels(\n",
    "    [pretty_feature_names_dict[x[0]] for x in res_sorted.items()],\n",
    "    rotation = 90,\n",
    "    ha = 'center'\n",
    ")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "l2 = ax2.bar(np.arange(n) + width, [x[1]['sample'] for x in res_sorted.items()], color = 'g',\n",
    "        width = width, edgecolor = 'black',\n",
    "        )\n",
    "\n",
    "plt.legend([l1, l2], [\"LLR\", \"N\"])\n",
    "plt.xlabel(\"Feature\")\n",
    "ax1.set_ylabel(\"Log-Likelihood Ratio (LLR)\")\n",
    "ax2.set_ylabel(\"Sample Size (N)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd87ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, dpi = 300, figsize = (7, 5), sharey = True)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    axs[i].plot(\n",
    "        best_model_features[model_name]['results']['added_feature'], \n",
    "        best_model_features[model_name]['results']['AIC'] * 1e7,\n",
    "        marker = 'o',\n",
    "        markersize = 3\n",
    "    )\n",
    "    \n",
    "    axs[i].set_xticklabels(\n",
    "        [pretty_feature_names_dict[x] for x in best_model_features[model_name]['results']['added_feature']],\n",
    "        rotation = 90\n",
    "    )\n",
    "    axs[i].set_title(model_name_legend_label_dict[model_name])\n",
    "    axs[i].grid()\n",
    "    \n",
    "\n",
    "axs[0].set_ylabel('AIC ($\\\\times 10^7$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf000e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58281329",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_results_df = pd.DataFrame(\n",
    "    columns = [\n",
    "        'cohort',\n",
    "        'model',\n",
    "        'cutoff',\n",
    "        'method',\n",
    "        'n_patients',\n",
    "        'features',\n",
    "        'n_features',\n",
    "        'HR',\n",
    "        'HR_lowerbound',\n",
    "        'HR_upperbound',\n",
    "        'p', \n",
    "        'c-index'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for cohort in ['training', 'validation']:\n",
    "    for model_name in model_names:\n",
    "        for cutoff_level in cutoff_levels:\n",
    "            results = survival_results_data[cohort][model_name][cutoff_level]\n",
    "                                                                \n",
    "            if cohort == 'training':\n",
    "                survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': cutoff_level,\n",
    "                    'method': \"OS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['OS']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['OS']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['OS']['HR']['Upper Bound'],\n",
    "                    'p': results['OS']['p'],\n",
    "                    'c-index': results['OS']['c-index']\n",
    "                }, ignore_index = True)\n",
    "            elif cohort == 'validation':\n",
    "                #RFI\n",
    "                survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': cutoff_level,\n",
    "                    'method': \"RFI\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['Univariate']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['Univariate']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['Univariate']['HR']['Upper Bound'],\n",
    "                    'p': results['Univariate']['p'],\n",
    "                    'c-index': results['Univariate']['c-index']\n",
    "                }, ignore_index = True)\n",
    "                \n",
    "                #RFS\n",
    "                survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': cutoff_level,\n",
    "                    'method': \"RFS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['RFS']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['RFS']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['RFS']['HR']['Upper Bound'],\n",
    "                    'p': results['RFS']['p'],\n",
    "                    'c-index': results['RFS']['c-index']\n",
    "                }, ignore_index = True)\n",
    "                \n",
    "                #OS\n",
    "                survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': cutoff_level,\n",
    "                    'method': \"OS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['OS']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['OS']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['OS']['HR']['Upper Bound'],\n",
    "                    'p': results['OS']['p'],\n",
    "                    'c-index': results['OS']['c-index']\n",
    "                }, ignore_index = True)\n",
    "                \n",
    "for cohort in ['training', 'validation']:\n",
    "    for model_name in model_names:   \n",
    "        results = survival_results_data[cohort][model_name]['95% Sensitivity']\n",
    "        \n",
    "        if cohort == 'training':\n",
    "            survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': 'Raw Model Prediction',\n",
    "                    'method': \"OS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['OS_predict']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['OS_predict']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['OS_predict']['HR']['Upper Bound'],\n",
    "                    'p': results['OS_predict']['p'],\n",
    "                    'c-index': results['OS_predict']['c-index']\n",
    "                }, ignore_index = True)\n",
    "        elif cohort == 'validation':\n",
    "            ##RFI\n",
    "            survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': 'Raw Model Prediction',\n",
    "                    'method': \"RFI\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['RFI_predict']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['RFI_predict']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['RFI_predict']['HR']['Upper Bound'],\n",
    "                    'p': results['RFI_predict']['p'],\n",
    "                    'c-index': results['RFI_predict']['c-index']\n",
    "                }, ignore_index = True)\n",
    "            \n",
    "            ##RFS\n",
    "            survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': 'Raw Model Prediction',\n",
    "                    'method': \"RFS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['RFS_predict']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['RFS_predict']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['RFS_predict']['HR']['Upper Bound'],\n",
    "                    'p': results['RFS_predict']['p'],\n",
    "                    'c-index': results['RFS_predict']['c-index']\n",
    "                }, ignore_index = True)\n",
    "            \n",
    "            ##OS\n",
    "            survival_results_df = survival_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'cohort': cohort,\n",
    "                    'cutoff': 'Raw Model Prediction',\n",
    "                    'method': \"OS\",\n",
    "                    'n_patients': results['Number of Patients'],\n",
    "                    'features': results['Features'],\n",
    "                    'n_features': len(results['Features']),\n",
    "                    'HR': results['OS_predict']['HR']['HR'],\n",
    "                    'HR_lowerbound': results['OS_predict']['HR']['Lower Bound'],\n",
    "                    'HR_upperbound': results['OS_predict']['HR']['Upper Bound'],\n",
    "                    'p': results['OS_predict']['p'],\n",
    "                    'c-index': results['OS_predict']['c-index']\n",
    "                }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ae38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_results_df = pd.DataFrame(\n",
    "    columns = [\n",
    "        'model',\n",
    "        'subgroup_type',\n",
    "        'subgroup_group',\n",
    "        'n_patients',\n",
    "        'features',\n",
    "        'n_features',\n",
    "        'cutoff',\n",
    "        'thresh',\n",
    "        'sens',\n",
    "        'spec',\n",
    "        'ppv',\n",
    "        'npv',\n",
    "        'AUC',\n",
    "        'AUC_lowerbound',\n",
    "        'AUC_upperbound',\n",
    "        'AUPRC',\n",
    "        'AUPRC_lowerbound',\n",
    "        'AUPRC_upperbound'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for model_name in model_names:\n",
    "    for subgroup_type in best_model_features[model_name]['subgroup_analyses'].keys():\n",
    "        for subgroup_group in best_model_features[model_name]['subgroup_analyses'][subgroup_type].keys():\n",
    "            print(model_name, subgroup_type, subgroup_group)\n",
    "            results = best_model_features[model_name]['subgroup_analyses'][subgroup_type][subgroup_group]['senspec_results']\n",
    "            \n",
    "            if results != None:\n",
    "                features = results['Features']\n",
    "                n_patients = results['Number of Patients']\n",
    "                n_features = len(features)\n",
    "                auc = results['AUC']['AUC']\n",
    "                auc_lb = results['AUC']['Lower Bound']\n",
    "                auc_ub = results['AUC']['Upper Bound']\n",
    "                auprc = results['AUPRC']['AUPRC']\n",
    "                auprc_lb = results['AUPRC']['Lower Bound']\n",
    "                auprc_ub = results['AUPRC']['Upper Bound']\n",
    "\n",
    "                for cutoff_level in [\n",
    "                    '95% Sensitivity',\n",
    "                    '90% Sensitivity',\n",
    "                    '95% Specificity',\n",
    "                    '90% Specificity'\n",
    "                ]:\n",
    "                    subgroup_results_df = subgroup_results_df.append({\n",
    "                        'model': model_name,\n",
    "                        'subgroup_type': subgroup_type,\n",
    "                        'subgroup_group': subgroup_group,\n",
    "                        'cutoff': cutoff_level,\n",
    "                        'n_patients': n_patients,\n",
    "                        'features': features,\n",
    "                        'n_features': n_features,\n",
    "                        'thresh': results[cutoff_level]['Threshold'],\n",
    "                        'sens': results[cutoff_level]['Sensitivity'],\n",
    "                        'spec': results[cutoff_level]['Specificity'],\n",
    "                        'ppv': results[cutoff_level]['PPV'],\n",
    "                        'npv': results[cutoff_level]['NPV'],\n",
    "                        'AUC': auc,\n",
    "                        'AUC_lowerbound': auc_lb,\n",
    "                        'AUC_upperbound': auc_ub,\n",
    "                        'AUPRC': auprc,\n",
    "                        'AUPRC_lowerbound': auprc_lb,\n",
    "                        'AUPRC_upperbound': auprc_ub\n",
    "                    }, ignore_index = True)\n",
    "            else:\n",
    "                subgroup_results_df = subgroup_results_df.append({\n",
    "                    'model': model_name,\n",
    "                    'subgroup_type': subgroup_type,\n",
    "                    'subgroup_group': subgroup_group,\n",
    "                    'cutoff': cutoff_level,\n",
    "                    'n_patients': None,\n",
    "                    'features': None,\n",
    "                    'n_features': None,\n",
    "                    'thresh': None,\n",
    "                    'sens': None,\n",
    "                    'spec': None,\n",
    "                    'ppv': None,\n",
    "                    'npv': None,\n",
    "                    'AUC': None,\n",
    "                    'AUC_lowerbound': None,\n",
    "                    'AUC_upperbound': None,\n",
    "                    'AUPRC': None,\n",
    "                    'AUPRC_lowerbound': None,\n",
    "                    'AUPRC_upperbound': None\n",
    "                }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e7a4f",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f6981",
   "metadata": {},
   "source": [
    "### Histologic Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a23046",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_histologic_aucs = (subgroup_results_df[\n",
    "    (subgroup_results_df.subgroup_type == 'histologic') &\n",
    "    (subgroup_results_df.cutoff == \"95% Sensitivity\")\n",
    "].pivot(\n",
    "    values = ['AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['cutoff'],\n",
    "    index = ['model', 'subgroup_group']\n",
    ")\n",
    " .swaplevel(0, 1, axis = 1)\n",
    " .droplevel(level = 0, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .astype(str)\n",
    ")\n",
    "\n",
    "subgroup_histologic_aucs['AUC (CI)'] = make_ci_text(subgroup_histologic_aucs, label = 'AUC')\n",
    "subgroup_histologic_aucs['AUPRC (CI)'] = make_ci_text(subgroup_histologic_aucs, label = 'AUPRC')\n",
    "\n",
    "subgroup_histologic_aucs.loc[['No Quant', 'ER/PR%', 'ER/PR%+Ki67%'], ['AUC (CI)', 'AUPRC (CI)']].rename(columns = {'AUC (CI)': 'AUROC (CI)'}).rename(model_name_legend_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2908be4",
   "metadata": {},
   "source": [
    "### Racial Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_race_aucs = (subgroup_results_df[\n",
    "    (subgroup_results_df.subgroup_type == 'race') &\n",
    "    (subgroup_results_df.cutoff == \"95% Sensitivity\") &\n",
    "    (subgroup_results_df.subgroup_group.isin(['Asian', 'Hispanic', 'Non-Hispanic Black', 'Non-Hispanic White']))\n",
    "].pivot(\n",
    "    values = ['AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['cutoff'],\n",
    "    index = ['model', 'subgroup_group']\n",
    ")\n",
    " .swaplevel(0, 1, axis = 1)\n",
    " .droplevel(level = 0, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .astype(str)\n",
    ")\n",
    "\n",
    "subgroup_race_aucs['AUC (CI)'] = make_ci_text(subgroup_race_aucs, label = 'AUC')\n",
    "subgroup_race_aucs['AUPRC (CI)'] = make_ci_text(subgroup_race_aucs, label = 'AUPRC')\n",
    "\n",
    "subgroup_race_aucs.loc[['No Quant', 'ER/PR%', 'ER/PR%+Ki67%'], ['AUC (CI)', 'AUPRC (CI)']].rename(columns = {'AUC (CI)': 'AUROC (CI)'}).rename(model_name_legend_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d75dd0",
   "metadata": {},
   "source": [
    "### Nodal Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_nodes_aucs = (subgroup_results_df[\n",
    "    (subgroup_results_df.subgroup_type == 'nodes') &\n",
    "    (subgroup_results_df.cutoff == \"95% Sensitivity\")\n",
    "].pivot(\n",
    "    values = ['AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['cutoff'],\n",
    "    index = ['model', 'subgroup_group']\n",
    ")\n",
    " .swaplevel(0, 1, axis = 1)\n",
    " .droplevel(level = 0, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .astype(str)\n",
    ")\n",
    "\n",
    "subgroup_nodes_aucs['AUC (CI)'] = make_ci_text(subgroup_nodes_aucs, label = 'AUC')\n",
    "subgroup_nodes_aucs['AUPRC (CI)'] = make_ci_text(subgroup_nodes_aucs, label = 'AUPRC')\n",
    "\n",
    "subgroup_nodes_aucs.loc[['No Quant', 'ER/PR%', 'ER/PR%+Ki67%'], ['AUC (CI)', 'AUPRC (CI)']].rename(columns = {'AUC (CI)': 'AUROC (CI)'}).rename(model_name_legend_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41e097",
   "metadata": {},
   "source": [
    "### Overall Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82475338",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_aucs = (subgroup_results_df[\n",
    "    ((subgroup_results_df.subgroup_group == 'test') |\n",
    "    (subgroup_results_df.subgroup_group == 'external')) & \n",
    "    (subgroup_results_df.cutoff == '90% Sensitivity')\n",
    "].pivot(\n",
    "    values = ['AUC', 'AUC_lowerbound', 'AUC_upperbound', 'AUPRC', 'AUPRC_lowerbound', 'AUPRC_upperbound'],\n",
    "    columns = ['subgroup_group'],\n",
    "    index = ['model']\n",
    ")\n",
    " .swaplevel(0, 1, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .reindex(model_names)\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ba303",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in ['test', 'external']:\n",
    "    for m_i in ['AUC', 'AUPRC']:\n",
    "        compare_aucs[(cohort, m_i + ' (CI)')] = compare_aucs[(cohort, m_i)] + \" (\" + compare_aucs[(cohort, m_i + '_lowerbound')] + \" - \" + compare_aucs[(cohort, m_i + '_upperbound')] + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35650f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(compare_aucs.swaplevel(0,1, axis = 1)[['AUC (CI)', 'AUPRC (CI)']]\n",
    " .swaplevel(0,1, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .rename(columns = {'test': 'NCDB', 'external': 'UCMC', 'AUC (CI)': 'AUROC (CI)'})\n",
    " .rename(model_name_legend_label_dict)\n",
    ")[['NCDB', 'UCMC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6acbb",
   "metadata": {},
   "source": [
    "### Utility of Models as Rule-Out Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(subgroup_results_df[\n",
    "    ((subgroup_results_df.subgroup_group == 'test') |\n",
    "    (subgroup_results_df.subgroup_group == 'external')) & \n",
    "    ((subgroup_results_df.cutoff == '90% Sensitivity') |\n",
    "    (subgroup_results_df.cutoff == '95% Sensitivity'))\n",
    "].pivot(\n",
    "    values = ['sens', 'spec', 'ppv', 'npv'],\n",
    "    columns = ['subgroup_group'],\n",
    "    index = ['cutoff', 'model']\n",
    ").swaplevel(0, 1, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .reindex(model_names, level = 'model')\n",
    " .reindex(['95% Sensitivity', '90% Sensitivity'], level = 'cutoff')\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .rename(columns={'test': 'NCDB', 'external': 'UCMC', 'npv': 'NPV', 'ppv': 'PPV', 'sens': 'Sen', 'spec': 'Spe'})\n",
    " .rename(model_name_legend_label_dict)\n",
    " .reindex(['Sen', 'Spe', 'PPV', 'NPV'], axis=1, level=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83ca3e",
   "metadata": {},
   "source": [
    "### Survival Analysis in UCMC Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81516ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "survival_results_df_pivot = (survival_results_df.pivot(\n",
    "    values = ['HR', 'HR_lowerbound', 'HR_upperbound', 'p', 'c-index'],\n",
    "    columns = ['cohort', 'method'],\n",
    "    index = ['cutoff', 'model']\n",
    ")\n",
    " .swaplevel(0, 1, axis = 1).swaplevel(1, 2, axis = 1).sort_index(axis = 1)\n",
    " .applymap(lambda x: round(x, 2) if not pd.isna(x) else x)\n",
    " .astype(str)\n",
    ")\n",
    "\n",
    "survival_results_df_pivot[('training', 'OS', 'HR (CI)')] = survival_results_df_pivot[('training', 'OS', 'HR')] + \" (\" + survival_results_df_pivot[('training', 'OS', 'HR_lowerbound')] + \" - \" + survival_results_df_pivot[('training', 'OS', 'HR_upperbound')] + \")\"\n",
    "\n",
    "for m_i in ['OS', 'RFI', 'RFS']:\n",
    "    survival_results_df_pivot[('validation', m_i, 'HR (CI)')] = survival_results_df_pivot[('validation', m_i, 'HR')] + \" (\" + survival_results_df_pivot[('validation', m_i, 'HR_lowerbound')] + \" - \" + survival_results_df_pivot[('validation', m_i, 'HR_upperbound')] + \")\"\n",
    "\n",
    "(survival_results_df_pivot.swaplevel(0,2, axis = 1)[['HR (CI)', 'p', 'c-index']]\n",
    " .swaplevel(0,2, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .reindex(model_names, level = 1)\n",
    " .reindex(['RFI', 'RFS', 'OS'], level = 1, axis = 1).\n",
    " rename(model_name_legend_label_dict)\n",
    ")['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40234fee",
   "metadata": {},
   "source": [
    "### Survival Analysis in NCDB Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "(survival_results_df_pivot.swaplevel(0,2, axis = 1)[['HR (CI)', 'p', 'c-index']]\n",
    " .swaplevel(0,2, axis = 1)\n",
    " .sort_index(axis = 1)\n",
    " .reindex(model_names, level = 1)\n",
    " .reindex(['RFI', 'RFS', 'OS'], level = 1, axis = 1)\n",
    " .rename(model_name_legend_label_dict)\n",
    ")['training']['OS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a700e5",
   "metadata": {},
   "source": [
    "### Z-scores and p-values for comparison of AUROC in validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_compare_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_compare_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e01a4d",
   "metadata": {},
   "source": [
    "### Median follow-up time in test and validation cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6eb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ucmc_parse_survival_val['year_FU'].median()*12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1eb26",
   "metadata": {},
   "source": [
    "# Pickles for Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dir = '/home/asim/NCDB_Projects/NCDBRS-main/pickle/'\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = LogisticRegression(max_iter = 1000, n_jobs = -1)\n",
    "    df_subset_train = df_train[best_model_features[model_name]['features'] + ['high_odx']]\n",
    "    model.fit(df_subset_train[best_model_features[model_name]['features']], df_subset_train['high_odx'])\n",
    "    \n",
    "    with open(pickle_dir + model_name_legend_file_dict[model_name] + '.pickle', 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_dir + 'model_features.pickle', 'wb') as f:\n",
    "    pickle.dump(best_model_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model_features['No Quant']['subgroup_analyses']['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    feat_dict[model_name_legend_file_dict[model_name]] = best_model_features[model_name]['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROCFeatExternal_pickle(\n",
    "    df_train, df_ucmc_parse_odx_val, feat_dict,\n",
    "    model = model,\n",
    "    pickle_dir = pickle_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROCFeat_pickle(\n",
    "    pd.concat([df_train, df_test_ncdb]), feat_dict,\n",
    "    model = model,\n",
    "    pickle_dir = pickle_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_survival_pickle(\n",
    "    df_train, df_ucmc_parse_survival_val, feat_dict,\n",
    "    model = model,\n",
    "    pickle_dir = pickle_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51724c",
   "metadata": {},
   "source": [
    "# Gridsearch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701603ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_dir = '/home/asim/NCDB_Projects/test/gridsearch/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearches = {\n",
    "    'Logistic Regression': gridsearch_fromfile(gridsearch_dir + 'logistic_regression.txt'),\n",
    "    'Random Forest': gridsearch_fromfile(gridsearch_dir + 'random_forest.txt'),\n",
    "    'Adaboost': gridsearch_fromfile(gridsearch_dir + 'adaboost.txt'),\n",
    "    'Neural Network: 1 Layer': gridsearch_fromfile(gridsearch_dir + 'neuralnetwork1.txt'),\n",
    "    'Neural Network: 2 Layers': gridsearch_fromfile(gridsearch_dir + 'neuralnetwork2.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2179dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in gridsearches.items():\n",
    "    print(k.upper())\n",
    "    print('Best Parameters:')\n",
    "    print(v.loc[v['AUC'].idxmax(), 'Parameters'])\n",
    "    print('AUC: %.3f, AIC: %.3e' % (v.loc[v['AUC'].idxmax(), 'AUC'], v.loc[v['AUC'].idxmax(), 'AIC']))\n",
    "    print('Features: %s' % str(v.loc[v['AUC'].idxmax(), 'Features']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030736b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f4fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320f28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bac40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ce96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a16f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8eab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b8f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b0a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf1b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f9ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncdb",
   "language": "python",
   "name": "ncdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
